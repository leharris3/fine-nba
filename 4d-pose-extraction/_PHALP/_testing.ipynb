{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from phalp.utils import get_pylogger\n",
    "from PHALP_MOD import PHALP\n",
    "from HMAR_MOD import HMAR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "log = get_pylogger(__name__)\n",
    "\n",
    "# load config\n",
    "cfg_fp = \"/mnt/arc/levlevi/nba-positions-videos-dataset/4d-pose-extraction/PHALP/scripts/config.json\"\n",
    "with open(cfg_fp, \"r\") as f:\n",
    "    cfg = DictConfig(json.load(f))\n",
    "\n",
    "# load PHALP obj\n",
    "phalp_tracker = PHALP(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Dict\n",
    "from phalp.utils.utils import (\n",
    "    smpl_to_pose_camera_vector,\n",
    ")\n",
    "\n",
    "BBX_FRAME_IDX_IDX = 0\n",
    "BBX_BBX_IDX = 1\n",
    "\n",
    "def get_measurements(img_frame: np.array):\n",
    "    img_height, img_width, _ = img_frame.shape\n",
    "    new_image_size = max(img_height, img_width)\n",
    "    top, left = (\n",
    "        (new_image_size - img_height) // 2,\n",
    "        (new_image_size - img_width) // 2,\n",
    "    )\n",
    "    measurments = [img_height, img_width, new_image_size, left, top]\n",
    "    return measurments\n",
    "\n",
    "\n",
    "def get_segmentation_mask(img: np.array, bbx: List[float]) -> np.array:\n",
    "    \"\"\"\n",
    "    Return a segmentation mask that is `True` where ever a bbx is contained within an image.\n",
    "\n",
    "    Params\n",
    "    : img: np.array     (H, W, 3)\n",
    "    \n",
    "    TODO: this function is useless,\n",
    "    we need an actual segmentation mask that wraps around a person's outline!\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    assert width > height, f\"failed a sanity check, height > width\"\n",
    "\n",
    "    x1, y1, x2, y2 = bbx\n",
    "    all_false_img_arr = np.zeros((height, width))\n",
    "\n",
    "    # set all values within bounding box region to 1.0\n",
    "    all_false_img_arr[y1:y2, x1:x2] = 1.0\n",
    "\n",
    "    # cast to bool type (t/f)\n",
    "    segmentation_mask = all_false_img_arr.astype(bool)\n",
    "    return segmentation_mask\n",
    "\n",
    "\n",
    "def pre_process_bounding_boxes(\n",
    "    phalp: PHALP, num_bbxs: int, image: np.array, bbxs: np.array\n",
    "):\n",
    "\n",
    "    masked_image_list = []\n",
    "    center_list = []\n",
    "    scale_list = []\n",
    "    rles_list = []\n",
    "    bounding_box_ids = []\n",
    "\n",
    "    # pre-process batch of bounding boxes for an img\n",
    "    for bbx_idx in range(num_bbxs):\n",
    "        # TODO: verify that this indexing works\n",
    "        bbox = bbxs[bbx_idx]\n",
    "        # by default we do no padding, so these two objs are identical\n",
    "        bbox_pad = bbox\n",
    "        seg_mask = get_segmentation_mask(image, bbox)\n",
    "        # min/max bbx size threshold | throw out small bbxs\n",
    "        if (\n",
    "            bbox[2] - bbox[0] < phalp.cfg.phalp.small_w\n",
    "            or bbox[3] - bbox[1] < phalp.cfg.phalp.small_h\n",
    "        ):\n",
    "            continue\n",
    "        # crop the entire image about the bounding box\n",
    "        masked_image, _, _, rles, center_pad, scale_pad = phalp.get_croped_image(\n",
    "            image, bbox, bbox_pad, seg_mask\n",
    "        )\n",
    "        masked_image_list.append(masked_image)\n",
    "        center_list.append(center_pad)\n",
    "        scale_list.append(scale_pad)\n",
    "        rles_list.append(rles)\n",
    "        bounding_box_ids.append(bbx_idx)\n",
    "\n",
    "    return masked_image_list, center_list, scale_list, rles_list, bounding_box_ids\n",
    "\n",
    "\n",
    "def post_process_hmar_results(\n",
    "    hmar_out,\n",
    "    phalp: PHALP,\n",
    "    batch_size: int,\n",
    "    center_list: List,\n",
    "    scale_list: List,\n",
    "    left,\n",
    "    top,\n",
    "    ratio,\n",
    "):\n",
    "\n",
    "    uv_vector = hmar_out[\"uv_vector\"]\n",
    "\n",
    "    # quickly calculate the appearance embedding\n",
    "    appe_embedding = phalp.HMAR.autoencoder_hmar(uv_vector, en=True)\n",
    "    appe_embedding = appe_embedding.view(appe_embedding.shape[0], -1)\n",
    "\n",
    "    # simple data transform method, very fast\n",
    "    pred_smpl_params, pred_joints_2d, pred_joints, pred_cam = (\n",
    "        phalp.HMAR.get_3d_parameters(\n",
    "            hmar_out[\"pose_smpl\"],\n",
    "            hmar_out[\"pred_cam\"],\n",
    "            center=(np.array(center_list) + np.array([left, top])) * ratio,\n",
    "            img_size=phalp.cfg.render.res,\n",
    "            scale=np.max(np.array(scale_list), axis=1, keepdims=True) * ratio,\n",
    "        )\n",
    "    )\n",
    "    pred_smpl_params = [\n",
    "        {k: v[i].cpu().numpy() for k, v in pred_smpl_params.items()}\n",
    "        for i in range(batch_size)\n",
    "    ]\n",
    "\n",
    "    if phalp.cfg.phalp.pose_distance == \"joints\":\n",
    "        pose_embedding = pred_joints.cpu().view(batch_size, -1)\n",
    "    elif phalp.cfg.phalp.pose_distance == \"smpl\":\n",
    "        pose_embedding = []\n",
    "        for i in range(batch_size):\n",
    "            pose_embedding_ = smpl_to_pose_camera_vector(\n",
    "                pred_smpl_params[i], pred_cam[i]\n",
    "            )\n",
    "            pose_embedding.append(torch.from_numpy(pose_embedding_[0]))\n",
    "        pose_embedding = torch.stack(pose_embedding, dim=0)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown pose distance\")\n",
    "    pred_joints_2d_ = pred_joints_2d.reshape(batch_size, -1) / phalp.cfg.render.res\n",
    "    pred_cam_ = pred_cam.view(batch_size, -1)\n",
    "    pred_joints_2d_.contiguous()\n",
    "    pred_cam_.contiguous()\n",
    "    loca_embedding = torch.cat((pred_joints_2d_, pred_cam_, pred_cam_, pred_cam_), 1)\n",
    "\n",
    "    # keeping it here for legacy reasons (T3DP), but it is not used.\n",
    "    full_embedding = torch.cat(\n",
    "        (appe_embedding.cpu(), pose_embedding, loca_embedding.cpu()), 1\n",
    "    )\n",
    "    return (\n",
    "        appe_embedding,\n",
    "        pose_embedding,\n",
    "        loca_embedding,\n",
    "        uv_vector,\n",
    "        full_embedding,\n",
    "        pred_smpl_params,\n",
    "        pred_cam,\n",
    "        pred_joints,\n",
    "        pred_joints_2d,\n",
    "    )\n",
    "\n",
    "\n",
    "def format_results(\n",
    "    bbxs: np.array,\n",
    "    bounding_box_ids,\n",
    "    rles_list,\n",
    "    appe_embedding,\n",
    "    pose_embedding,\n",
    "    loca_embedding,\n",
    "    uv_vector,\n",
    "    full_embedding,\n",
    "    center_list,\n",
    "    scale_list,\n",
    "    pred_smpl_params,\n",
    "    pred_cam_,\n",
    "    hmar_out,\n",
    "    pred_joints,\n",
    "    pred_joints_2d_,\n",
    ") -> List[Dict]:\n",
    "    detection_data_list = []\n",
    "    for i, bbx_idx in enumerate(bounding_box_ids):\n",
    "        detection_data = {\n",
    "            \"bbox\": np.array(\n",
    "                [\n",
    "                    bbxs[bbx_idx][0],\n",
    "                    bbxs[bbx_idx][1],\n",
    "                    (bbxs[bbx_idx][2] - bbxs[bbx_idx][0]),\n",
    "                    (bbxs[bbx_idx][3] - bbxs[bbx_idx][1]),\n",
    "                ]\n",
    "            ),\n",
    "            \"mask\": rles_list[i],\n",
    "            \"appe\": appe_embedding[i].cpu().numpy(),\n",
    "            \"pose\": pose_embedding[i].numpy(),\n",
    "            \"loca\": loca_embedding[i].cpu().numpy(),\n",
    "            \"uv\": uv_vector[i].cpu().numpy(),\n",
    "            \"embedding\": full_embedding[i],\n",
    "            \"center\": center_list[i],\n",
    "            \"scale\": scale_list[i],\n",
    "            \"smpl\": pred_smpl_params[i],\n",
    "            \"camera\": pred_cam_[i].cpu().numpy(),\n",
    "            \"camera_bbox\": hmar_out[\"pred_cam\"][i].cpu().numpy(),\n",
    "            \"3d_joints\": pred_joints[i].cpu().numpy(),\n",
    "            \"2d_joints\": pred_joints_2d_[i].cpu().numpy(),\n",
    "        }\n",
    "        detection_data_list.append(detection_data)\n",
    "    return detection_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "1. verify outputs\n",
    "2. benchmark (is it fast enough?)\n",
    "3. write full pipeline and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "from PHALP_MOD import PHALP\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_3d_poses(\n",
    "    phalp: PHALP,\n",
    "    img: np.array,\n",
    "    bbxs: np.array,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Params\n",
    "    N: # bounding-boxes\n",
    "    :img: np.array  (H, W, 3)\n",
    "    :bbxs: np.array (N, 4) | [[ x1, y1, x2, y2]]\n",
    "    \"\"\"\n",
    "\n",
    "    NUM_BBXS = bbxs.shape[0]\n",
    "    if NUM_BBXS == 0:\n",
    "        log.warn(f\"A bbxs with dim 0 == 0 passed to `predict_3d_poses`\")\n",
    "        return []\n",
    "\n",
    "    _, _, new_img_size, img_left, img_top = get_measurements(img)\n",
    "\n",
    "    # used later for calculating some other vars\n",
    "    img_ratio = 1.0 / int(new_img_size) * phalp.cfg.render.res\n",
    "\n",
    "    # pre-process images and bouding boxes\n",
    "    masked_image_list, center_list, scale_list, rles_list, bounding_box_ids = (\n",
    "        pre_process_bounding_boxes(phalp, NUM_BBXS, img, bbxs)\n",
    "    )\n",
    "\n",
    "    log.info(\"PHALP: masked_image_list {}\".format(len(masked_image_list)))\n",
    "\n",
    "    if len(masked_image_list) == 0:\n",
    "        log.error(f\"No masked images generated for a non-empty input set\")\n",
    "        raise Exception\n",
    "        return []\n",
    "\n",
    "    # tensor of shape (N, H, W, 3)\n",
    "    masked_image_list = torch.stack(masked_image_list, dim=0)\n",
    "    batch_size = masked_image_list.size(0)\n",
    "\n",
    "    log.debug(\"HMAR forward pass... \")\n",
    "\n",
    "    # TODO: HMAR forward pass\n",
    "    # this function appears to accept ONE IMAGE but UNLIMITED BBXS per forward pass (N, H, W, 3)\n",
    "    start = time.time()\n",
    "    hmar_out = phalp.HMAR(masked_image_list.cuda(), **{})\n",
    "    log.info(\"PHALP: HMAR forward pass took {} seconds\".format(time.time() - start))\n",
    "\n",
    "    start = time.time()\n",
    "    log.debug(\"PHALP: hmar_out {}\".format(hmar_out.keys()))\n",
    "\n",
    "    # post-process results of the HMAR forward pass\n",
    "    (\n",
    "        appe_embedding,\n",
    "        pose_embedding,\n",
    "        loca_embedding,\n",
    "        uv_vector,\n",
    "        full_embedding,\n",
    "        pred_smpl_params,\n",
    "        pred_cam,\n",
    "        pred_joints,\n",
    "        pred_joints_2d,\n",
    "    ) = post_process_hmar_results(\n",
    "        hmar_out,\n",
    "        phalp,\n",
    "        batch_size,\n",
    "        center_list,\n",
    "        scale_list,\n",
    "        img_left,\n",
    "        img_top,\n",
    "        img_ratio,\n",
    "    )\n",
    "\n",
    "    results = format_results(\n",
    "        bbxs,\n",
    "        bounding_box_ids,\n",
    "        rles_list,\n",
    "        appe_embedding,\n",
    "        pose_embedding,\n",
    "        loca_embedding,\n",
    "        uv_vector,\n",
    "        full_embedding,\n",
    "        center_list,\n",
    "        scale_list,\n",
    "        pred_smpl_params,\n",
    "        pred_cam,\n",
    "        hmar_out,\n",
    "        pred_joints,\n",
    "        pred_joints_2d,\n",
    "    )\n",
    "    log.info(\n",
    "        \"PHALP: the rest of the forward pass took {} seconds\".format(\n",
    "            time.time() - start\n",
    "        )\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_IDX = 1\n",
    "NUM_BBXS = 1\n",
    "seg_mask = np.ones((NUM_BBXS, 720, 1280)).astype(bool)\n",
    "\n",
    "results = predict_3d_poses(\n",
    "    phalp=phalp_tracker,\n",
    "    img=np.random.random((720, 1280, 3)),\n",
    "    bbxs=np.array([[0, 100, 100, 200]] * NUM_BBXS),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: call forward pass on a dummy input\n",
    "\n",
    "# TODO: new output format\n",
    "#    pred_bbox,  # [[x1, y1, x2, y2]] -- (NUM_BBXS, 4)\n",
    "#         pred_bbox_pad,  # IDENTICAL TO `pred_bbox`\n",
    "#         pred_masks,  # (NUM_BBXS, H, W) [False if out of BBX, True if in BBX]\n",
    "#         pred_scores,  # [ 1. ] * NUM_BBXS\n",
    "#         pred_classes,  # [0] * NUM_BBXS\n",
    "#         gt_tids,  # [1] * NUM_BBXS\n",
    "#         gt_annots,  # [[]] * NUM_BBXS\n",
    "\n",
    "# TODO: COMPLETE OUTPUT FORMAT\n",
    "# how long does a single forward pass take\n",
    "# i.e. do we need batch processing?\n",
    "\n",
    "# N: # BBXS\n",
    "#     image_frame,       (N, H, W, 3) # TODO: how do we handle duplicate frames efficently, using a dict (duh!)\n",
    "#     pred_masks,        (N, H, W)\n",
    "#     pred_bbox,         (N, 4)\n",
    "#     pred_bbox_pad,     (N, 4)\n",
    "#     pred_scores,       (1.0) * N\n",
    "#     frame_name,        (None) * N\n",
    "#     pred_classes,      (0) * N\n",
    "#     frame_idx,         (INT) * N\n",
    "#     measurments,       (N, 5)\n",
    "#     gt_tids,           (1) * N\n",
    "#     gt_annots,         (()) * N\n",
    "#     extra_data,        list(range(len(pred_scores))) * N"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phalp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
