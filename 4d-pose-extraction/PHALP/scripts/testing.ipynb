{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from phalp.utils import get_pylogger\n",
    "from PHALP_MOD import PHALP\n",
    "from HMAR_MOD import HMAR\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "log = get_pylogger(__name__)\n",
    "\n",
    "# load config\n",
    "cfg_fp = \"/mnt/arc/levlevi/nba-positions-videos-dataset/4d-pose-extraction/PHALP/scripts/config.json\"\n",
    "with open(cfg_fp, \"r\") as f:\n",
    "    cfg = DictConfig(json.load(f))\n",
    "\n",
    "# load PHALP obj\n",
    "phalp_tracker = PHALP(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "from phalp.external.deep_sort_.detection import Detection\n",
    "from phalp.utils.utils import (\n",
    "    smpl_to_pose_camera_vector,\n",
    ")\n",
    "\n",
    "\n",
    "def get_human_features(\n",
    "    phalp: PHALP,\n",
    "    image,\n",
    "    seg_mask,\n",
    "    bbox,\n",
    "    bbox_pad,\n",
    "    score,\n",
    "    frame_name,\n",
    "    cls_id,\n",
    "    frame_idx: int,\n",
    "    measurments,\n",
    "    gt=1,\n",
    "    ann=None,\n",
    "    extra_data=None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Given: an image and a set of bbxs.\n",
    "    Return: a list 3D pose predictions.\n",
    "    \n",
    "    img in -> 3d poses out\n",
    "\n",
    "    This by far the most complex and important function in this entire script.\n",
    "    \n",
    "    Can we re-write this function to support mutliple images?\n",
    "    \"\"\"\n",
    "\n",
    "    NPEOPLE = len(score)\n",
    "    if NPEOPLE == 0:\n",
    "        return []\n",
    "\n",
    "    img_height, img_width, new_image_size, left, top = measurments\n",
    "    # resize ratio\n",
    "    ratio = 1.0 / int(new_image_size) * phalp.cfg.render.res\n",
    "    \n",
    "    masked_image_list = []\n",
    "    center_list = []\n",
    "    scale_list = []\n",
    "    rles_list = []\n",
    "    selected_ids = []\n",
    "\n",
    "    # crop images\n",
    "    for p_ in range(NPEOPLE):\n",
    "        if (\n",
    "            bbox[p_][2] - bbox[p_][0] < phalp.cfg.phalp.small_w\n",
    "            or bbox[p_][3] - bbox[p_][1] < phalp.cfg.phalp.small_h\n",
    "        ):\n",
    "            continue\n",
    "        masked_image, center_, scale_, rles, center_pad, scale_pad = (\n",
    "            phalp.get_croped_image(image, bbox[p_], bbox_pad[p_], seg_mask[p_])\n",
    "        )\n",
    "\n",
    "        masked_image_list.append(masked_image)\n",
    "        center_list.append(center_pad)\n",
    "        scale_list.append(scale_pad)\n",
    "        rles_list.append(rles)\n",
    "        selected_ids.append(p_)\n",
    "\n",
    "    log.info(\"PHALP: masked_image_list {}\".format(len(masked_image_list)))\n",
    "\n",
    "    if len(masked_image_list) == 0:\n",
    "        return []\n",
    "\n",
    "    masked_image_list = torch.stack(masked_image_list, dim=0)\n",
    "    BS = masked_image_list.size(0)\n",
    "\n",
    "    # TODO: HMAR forward pass\n",
    "    with torch.no_grad():\n",
    "\n",
    "        extra_args = {}\n",
    "\n",
    "        # forward pass, bulk of computation occurs here\n",
    "        log.debug(\"Calculating HMAR forward pass\")\n",
    "        start = time.time()\n",
    "        hmar_out = phalp.HMAR(masked_image_list.cuda(), **extra_args)\n",
    "        log.info(\"PHALP: HMAR forward pass took {} seconds\".format(time.time() - start))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        log.debug(\"PHALP: hmar_out {}\".format(hmar_out.keys()))\n",
    "\n",
    "        uv_vector = hmar_out[\"uv_vector\"]\n",
    "\n",
    "        # something i don't understand that, very fast\n",
    "        appe_embedding = phalp.HMAR.autoencoder_hmar(uv_vector, en=True)\n",
    "\n",
    "        appe_embedding = appe_embedding.view(appe_embedding.shape[0], -1)\n",
    "\n",
    "        # simple data transform method, very fast\n",
    "        pred_smpl_params, pred_joints_2d, pred_joints, pred_cam = (\n",
    "            phalp.HMAR.get_3d_parameters(\n",
    "                hmar_out[\"pose_smpl\"],\n",
    "                hmar_out[\"pred_cam\"],\n",
    "                center=(np.array(center_list) + np.array([left, top])) * ratio,\n",
    "                img_size=phalp.cfg.render.res,\n",
    "                scale=np.max(np.array(scale_list), axis=1, keepdims=True) * ratio,\n",
    "            )\n",
    "        )\n",
    "        pred_smpl_params = [\n",
    "            {k: v[i].cpu().numpy() for k, v in pred_smpl_params.items()}\n",
    "            for i in range(BS)\n",
    "        ]\n",
    "\n",
    "        if phalp.cfg.phalp.pose_distance == \"joints\":\n",
    "            pose_embedding = pred_joints.cpu().view(BS, -1)\n",
    "        elif phalp.cfg.phalp.pose_distance == \"smpl\":\n",
    "            pose_embedding = []\n",
    "            for i in range(BS):\n",
    "                pose_embedding_ = smpl_to_pose_camera_vector(\n",
    "                    pred_smpl_params[i], pred_cam[i]\n",
    "                )\n",
    "                pose_embedding.append(torch.from_numpy(pose_embedding_[0]))\n",
    "            pose_embedding = torch.stack(pose_embedding, dim=0)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown pose distance\")\n",
    "        pred_joints_2d_ = pred_joints_2d.reshape(BS, -1) / phalp.cfg.render.res\n",
    "        pred_cam_ = pred_cam.view(BS, -1)\n",
    "        pred_joints_2d_.contiguous()\n",
    "        pred_cam_.contiguous()\n",
    "        loca_embedding = torch.cat(\n",
    "            (pred_joints_2d_, pred_cam_, pred_cam_, pred_cam_), 1\n",
    "        )\n",
    "\n",
    "    # keeping it here for legacy reasons (T3DP), but it is not used.\n",
    "    full_embedding = torch.cat(\n",
    "        (appe_embedding.cpu(), pose_embedding, loca_embedding.cpu()), 1\n",
    "    )\n",
    "\n",
    "    detection_data_list = []\n",
    "    for i, p_ in enumerate(selected_ids):\n",
    "        detection_data = {\n",
    "            \"bbox\": np.array(\n",
    "                [\n",
    "                    bbox[p_][0],\n",
    "                    bbox[p_][1],\n",
    "                    (bbox[p_][2] - bbox[p_][0]),\n",
    "                    (bbox[p_][3] - bbox[p_][1]),\n",
    "                ]\n",
    "            ),\n",
    "            \"mask\": rles_list[i],\n",
    "            \"conf\": score[p_],\n",
    "            \"appe\": appe_embedding[i].cpu().numpy(),\n",
    "            \"pose\": pose_embedding[i].numpy(),\n",
    "            \"loca\": loca_embedding[i].cpu().numpy(),\n",
    "            \"uv\": uv_vector[i].cpu().numpy(),\n",
    "            \"embedding\": full_embedding[i],\n",
    "            \"center\": center_list[i],\n",
    "            \"scale\": scale_list[i],\n",
    "            \"smpl\": pred_smpl_params[i],\n",
    "            \"camera\": pred_cam_[i].cpu().numpy(),\n",
    "            \"camera_bbox\": hmar_out[\"pred_cam\"][i].cpu().numpy(),\n",
    "            \"3d_joints\": pred_joints[i].cpu().numpy(),\n",
    "            \"2d_joints\": pred_joints_2d_[i].cpu().numpy(),\n",
    "            \"size\": [img_height, img_width],\n",
    "            \"img_path\": frame_name,\n",
    "            \"img_name\": (\n",
    "                frame_name.split(\"/\")[-1] if isinstance(frame_name, str) else None\n",
    "            ),\n",
    "            \"class_name\": cls_id[p_],\n",
    "            \"time\": frame_idx,\n",
    "            \"ground_truth\": gt[p_],\n",
    "            \"annotations\": ann[p_],\n",
    "            \"extra_data\": extra_data[p_] if extra_data is not None else None,\n",
    "        }\n",
    "        detection_data_list.append(Detection(detection_data))\n",
    "\n",
    "    log.info(\n",
    "        \"PHALP: the rest of the forward pass took {} seconds\".format(\n",
    "            time.time() - start\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return detection_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phalp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
