import numpy as np
import cv2
import random
import os
import pickle
import argparse
import torch
import torch.multiprocessing as mp

from pathlib import Path
from PIL import Image
from tqdm import tqdm
from glob import glob
from ultralytics import YOLO
from typing import Optional, List
from transformers.utils.logging import set_verbosity_error

from utils.draw_bbx import draw_bbx
from entities.clip_annotations import ClipAnnotation
from entities.clip_dataset import FilteredClipDataset

# silence pesky transformer lib warnings
set_verbosity_error()

ROI_MODEL_FP = "models/roi.pt"
TR_MODEL_FP = "/mnt/opr/levlevi/opr/fine-nba/src/statvu_align/runs/detect/roi-tr-ft-100/weights/best.pt"
YOLO_TIME_REMAINING_KEY = 1
DST_DIR = "/mnt/opr/levlevi/opr/fine-nba/src/statvu_align/__nba-plus-statvu-dataset__/statvu-aligned/raw-time-remaining-vals"


def resize_width(img: Image.Image, width: int, resample=None):
    """resize by width, keep ratio."""
    return img.resize((width, img.height * width // img.width), resample=resample)


def rec_text(ann: ClipAnnotation, roi, model: YOLO) -> str:
    """
    Given a batch of images return the predicted text values and confidence scores generated by `model`.
    """

    # video failed to load
    if ann.video_fp is None:
        return None, None, None

    # kind of bad, we get frames x2
    dst_dir = "/playpen-storage/levlevi/opr/fine-nba/src/statvu_align/runs/predictions"
    cropped_frames = []
    x1, y1, x2, y2 = list(map(int, roi))
    frames = ann.get_frames()

    # crop all frames
    for idx, img in enumerate(frames):
        cropped_img = img[y1:y2, x1:x2]
        im = Image.fromarray(cropped_img)
        im = resize_width(im, 320)
        cropped_img = np.array(im)
        cropped_frames.append(cropped_img)

    # yolo results obj
    results = model(
        cropped_frames,
        verbose=False,
        visualize=False,
        half=False,
        imgsz=320,
        save=False,
    )

    # save results objects as pkls
    ann_fp = Path(ann.annotations_fp)
    par_dir = ann_fp.parent.parent
    dst_dir = par_dir.__str__().replace(
        "filtered-clip-annotations-with-ratios-pkl", "statvu-aligned"
    )
    dst_fp = os.path.join(dst_dir, ann.basename + ".pkl")
    os.makedirs(dst_dir, exist_ok=True)
    with open(dst_fp, "wb") as f:
        pickle.dump(results, f)

    # print the resulting string in order L --> R
    # for idx, result in enumerate(results):
    #     boxes = result.boxes
    #     classes = boxes.cls.cpu().numpy().tolist()
    #     xyxy = boxes.xyxy.cpu().numpy().tolist()
    #     names = result.names
    #     # print(f"Classes: {classes}")
    #     # print(f"names: {names}")
    #     x1_locs = {}
    #     predicted_str = ""
    #     for c, bbx in zip(classes, xyxy):
    #         x1 = bbx[0]
    #         x1_locs[x1] = names[c]
    #     for k in sorted(x1_locs.keys()):
    #         predicted_str += x1_locs[k]
    #     print(f"Predicted string: {predicted_str}")
    #     annotated_res = result.plot()
    #     out_fp = os.path.join(dst_dir, f"{idx}_ann.png")
    #     cv2.imwrite(out_fp, annotated_res)


def get_roi(ann: ClipAnnotation, model: YOLO) -> Optional[List[float]]:
    """
    Given a `ClipAnnotation` obj, return the bbx coords of the roi containing
    the time-remaining values on the game clock.
    """

    # video failed to load
    if ann.video_fp is None:
        return None, None, None

    best_roi_xyxy = None
    best_roi_conf = 0.0
    best_img = None

    frames = ann.get_frames()
    results = model(frames, verbose=False)
    for result in results:
        boxes = result.boxes
        time_remaining_mask = boxes.cls.cpu().numpy() == YOLO_TIME_REMAINING_KEY
        if len(boxes.conf.cpu().numpy()[time_remaining_mask]) == 0:
            continue
        max_conf = boxes.conf.cpu().numpy()[time_remaining_mask].max()
        if max_conf > best_roi_conf:
            best_roi_conf = max_conf
            max_conf_idx = boxes.conf.cpu().numpy()[time_remaining_mask].argmax()
            best_roi_xyxy = boxes.xyxy.cpu().numpy()[time_remaining_mask][max_conf_idx]
            best_img = result.orig_img
    return best_roi_conf, best_roi_xyxy, np.array(best_img)


def sample_cropped_rois(
    filtered_clips: FilteredClipDataset, model: YOLO, num_samples: int, dst_dir: str
):
    assert os.path.isdir(dst_dir), f"Error: {dst_dir} DNE"
    assert num_samples > 0, f"Num samples must be greater than 0"

    annotations_fps = filtered_clips.filtered_clip_annotations_file_paths
    random.shuffle(annotations_fps)
    annotations_subset = annotations_fps[:num_samples]
    for fp in tqdm(
        annotations_subset, desc="Extracting Text ROIs", total=len(annotations_subset)
    ):
        ann = ClipAnnotation(fp)
        basename = ann.basename
        conf, roi, img = get_roi(ann, model)
        if roi is None:
            continue
        x1, y1, x2, y2 = roi.astype(int)
        cropped_img = img[y1:y2, x1:x2]
        out_fp = os.path.join(dst_dir, f"{basename}.png")
        cv2.imwrite(out_fp, cropped_img)


def main(rank: int):

    filtered_clip_dataset = FilteredClipDataset()
    roi_model = YOLO(ROI_MODEL_FP, verbose=False).to(rank)
    text_rec_model = YOLO(TR_MODEL_FP, verbose=False).to(rank)

    # take a 1/8 subset of the list
    clip_fps = filtered_clip_dataset.filtered_clip_annotations_file_paths[rank::8]

    for fp in tqdm(clip_fps, total=len(clip_fps), desc="Extracting Time Remaining"):
        ann = ClipAnnotation(fp)
        if not ann.video_fp:
            print(f"Error: no video found! Skipping annotation at {fp}")
            continue
        try:
            # find the text region
            _, roi, _ = get_roi(ann, roi_model)
            if roi is None:
                print(f"Error: no ROI found for fp: {fp}. Skipping!")
                continue
            # predict time remaining
            rec_text(ann, roi, text_rec_model)
        except Exception as e:
            print(f"Error: error raised while processing annotation at fp: {fp}")
            print(f"Error: {e}")
            continue


if __name__ == "__main__":
    mp.spawn(main, nprocs=8)
